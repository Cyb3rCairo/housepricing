# ğŸ“ˆ Multiple Linear Regression From Scratch (Python)

Este projeto implementa **Multiple Linear Regression do zero**, usando apenas **NumPy**, sem bibliotecas de machine learning como `scikit-learn`.

O objetivo **nÃ£o Ã© performance**, e sim **entendimento profundo**:
- como o modelo funciona
- como o erro Ã© calculado
- como o gradiente Ã© derivado
- como o Gradient Descent aprende de fato

Inspirado no curso **Machine Learning â€“ Andrew Ng**, mas aplicado em um **dataset real de preÃ§os de casas**.

---

## ğŸ§  O que Ã© feito neste projeto

ImplementaÃ§Ã£o manual de:

- Multiple Linear Regression
- NormalizaÃ§Ã£o das features (Z-score)
- FunÃ§Ã£o de custo (Mean Squared Error)
- CÃ¡lculo do gradiente
- Gradient Descent
- PrediÃ§Ã£o
- MÃ©trica de erro (RMSE)

Tudo **linha por linha**, sem atalhos.

---

## ğŸ“‚ Dataset

Dataset de preÃ§os de casas (`housepricing.csv`), contendo apenas **features numÃ©ricas** como:

- bedrooms  
- bathrooms  
- sqft_living  
- sqft_lot  
- floors  
- waterfront  
- view  
- condition  
- sqft_above  
- sqft_basement  
- yr_built  
- yr_renovated  

Target:
- `price`

Features categÃ³ricas (cidade, estado, endereÃ§o) **foram propositalmente ignoradas** para manter o foco no algoritmo.

---

## âš™ï¸ Estrutura do Modelo

O modelo aprendido Ã©:

\[
\$$hat{y} = w_1x_1 + w_2x_2 + \dots + w_nx_n + b$$
\]

Onde:
- `w` sÃ£o os pesos aprendidos
- `b` Ã© o bias
- `x` sÃ£o as features normalizadas

---

## ğŸ“ NormalizaÃ§Ã£o (Z-score)

Antes do treino, todas as features sÃ£o normalizadas:

\[
x_{norm} = \frac{x - \mu}{\sigma}
\]

Isso Ã© essencial para:
- evitar explosÃ£o do gradiente
- garantir convergÃªncia do Gradient Descent
- permitir um `alpha` estÃ¡vel

---

## ğŸ“‰ FunÃ§Ã£o de Custo

Usamos **Mean Squared Error**:

\[
J(w,b) = \frac{1}{2m} \sum (y - \hat{y})^2
\]

Essa funÃ§Ã£o **nÃ£o Ã© mÃ©trica de avaliaÃ§Ã£o**, e sim uma funÃ§Ã£o matemÃ¡tica usada para **otimizaÃ§Ã£o**.

---

## ğŸ§  Gradiente vs Gradient Descent

- **Gradiente**: indica a direÃ§Ã£o de maior aumento do erro  
- **Gradient Descent**: usa o gradiente para atualizar os parÃ¢metros e minimizar o erro

AtualizaÃ§Ã£o dos parÃ¢metros:

\[
w := w - \alpha \cdot \frac{\partial J}{\partial w}
\]

\[
b := b - \alpha \cdot \frac{\partial J}{\partial b}
\]

---

## ğŸ“Š AvaliaÃ§Ã£o do Modelo (RMSE)

Para interpretar o erro em unidades reais (dÃ³lares), usamos **RMSE**:

\[
RMSE = \sqrt{\frac{1}{m} \sum (y - \hat{y})^2}
\]

Isso responde:
> â€œEm mÃ©dia, quanto o modelo erra no preÃ§o de uma casa?â€

âš ï¸ O RMSE Ã© **apenas para avaliaÃ§Ã£o**, nÃ£o para treino.

---

## ğŸ§ª ObservaÃ§Ãµes Importantes

- O modelo Ã© **linear**
- O dataset Ã© **altamente nÃ£o-linear**
- NÃ£o hÃ¡ informaÃ§Ãµes de localizaÃ§Ã£o detalhada
- O erro alto Ã© esperado e faz parte do aprendizado

Este projeto **nÃ£o busca alta precisÃ£o**, e sim **compreensÃ£o do algoritmo**.

---

## ğŸš« O que NÃƒO foi usado

- scikit-learn
- `.fit()`
- `.predict()` prontos
- pipelines automÃ¡ticos
- mÃ©tricas prontas

Tudo foi implementado manualmente.

---

## ğŸ¯ Objetivo do Projeto

Este repositÃ³rio existe para:

- estudar Machine Learning de verdade
- entender o que acontece â€œpor baixo do capÃ´â€
- criar base sÃ³lida para modelos mais avanÃ§ados
- nÃ£o depender cegamente de bibliotecas

---

## ğŸ“Œ PrÃ³ximos Passos PossÃ­veis

- Train/Test split do zero
- RegularizaÃ§Ã£o (Ridge / Lasso) do zero
- VisualizaÃ§Ã£o da convergÃªncia do custo
- ComparaÃ§Ã£o com implementaÃ§Ã£o do `sklearn`
- ExtensÃ£o para redes neurais

---

## ğŸ§  Autor

Projeto de estudo pessoal com foco em **fundamentos reais de Machine Learning**.

Sem atalhos.
Sem mÃ¡gica.
SÃ³ matemÃ¡tica e cÃ³digo.
